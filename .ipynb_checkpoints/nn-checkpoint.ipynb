{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random starting synaptic weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "New synaptic weights after training: \n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "Considering new situation [1, 0, 0] -> ?: \n",
      "[ 0.99993704]\n"
     ]
    }
   ],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "import numpy as np \n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "\n",
    "        # We model a single neuron, with 3 input connections and 1 output connection.\n",
    "        # We assign random weights to a 3 x 1 matrix, with values in the range -1 to 1\n",
    "        # and mean 0.\n",
    "        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network (a single neuron).\n",
    "            output = self.think(training_set_inputs)\n",
    "\n",
    "            # Calculate the error (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            error = training_set_outputs - output\n",
    "\n",
    "            # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "            # This means less confident weights are adjusted more.\n",
    "            # This means inputs, which are zero, do not cause changes to the weights.\n",
    "            adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return self.__sigmoid(dot(inputs, self.synaptic_weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Intialise a single neuron neural network.\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Random starting synaptic weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set. We have 4 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "    training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "    # Train the neural network using a training set.\n",
    "    # Do it 10,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 10000)\n",
    "\n",
    "    print (\"New synaptic weights after training: \")\n",
    "    print( neural_network.synaptic_weights)\n",
    "\n",
    "    # Test the neural network with a new situation.\n",
    "    print (\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "    print (neural_network.think(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [2 2 2]\n",
      " [3 3 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 6, 12, 18])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.array([[1,2,3],[1,2,3],[1,2,3]])\n",
    "print(M.T)\n",
    "M.T.dot([3,2,1])\n",
    "\n",
    "# dot([1,2,3],np.array([[1,2,3],[1,2,3],[1,2,3]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [1, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_outputs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.009664  ],\n",
       "       [ 0.99211997],\n",
       "       [ 0.99358931],\n",
       "       [ 0.00786466]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network.think(training_set_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synaptic w8s:\n",
      "[[ 9.67299303]\n",
      " [-0.2078435 ]\n",
      " [-4.62963669]]\n",
      "training inputs:\n",
      "[[0 0 1]\n",
      " [0 0 1]\n",
      " [1 1 1]\n",
      " [1 0 1]\n",
      " [0 1 1]]\n",
      "outputs:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "training @dot synaptic w8\n",
      "[[-4.62963669]\n",
      " [-4.62963669]\n",
      " [ 4.83551284]\n",
      " [ 5.04335634]\n",
      " [-4.83748019]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "training_set_inputs = array([[0,0,1], [0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "synapic_weights = neural_network.synaptic_weights\n",
    "print('synaptic w8s:')\n",
    "print(synapic_weights)\n",
    "print('training inputs:')\n",
    "print(training_set_inputs)\n",
    "print('outputs:')\n",
    "print(training_set_outputs)\n",
    "tr_x_sw = dot(training_set_inputs, synapic_weights)\n",
    "print('training @dot synaptic w8')\n",
    "print(tr_x_sw)\n",
    "gotroughneuron = sigmoid(dot(training_set_inputs,synapic_weights)) \n",
    "# print(gotroughneuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9. ],\n",
       "       [-0.2],\n",
       "       [-4. ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapic_weights\n",
    "synapic_weights = np.array([9,-0.2,-4]).reshape(-1, 1)\n",
    "# synapic_weights.reshape(-1, 1).T\n",
    "\n",
    "training_set_inputs\n",
    "synapic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4. ],\n",
       "       [-4. ],\n",
       "       [ 4.8],\n",
       "       [ 5. ],\n",
       "       [-4.2]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_inputs.dot(synapic_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 2 * random.random((3, 1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9. ],\n",
       "       [-0.2],\n",
       "       [-4. ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([9,-0.2,-4]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9. ],\n",
       "       [-0.2],\n",
       "       [-4. ]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapic_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "[[ 0.12025406]\n",
      " [ 0.50456196]\n",
      " [-0.85063774]]\n",
      "[[ 0.40563039]\n",
      " [ 0.54131537]\n",
      " [-0.72857311]]\n",
      "[[ 0.65701265]\n",
      " [ 0.53906065]\n",
      " [-0.66099969]]\n",
      "[[ 0.86828909]\n",
      " [ 0.50813544]\n",
      " [-0.64314383]]\n",
      "[[ 1.04900401]\n",
      " [ 0.46321233]\n",
      " [-0.65628104]]\n",
      "[[ 1.20838473]\n",
      " [ 0.41369083]\n",
      " [-0.68564552]]\n",
      "[[ 1.35233482]\n",
      " [ 0.36453375]\n",
      " [-0.7224582 ]]\n",
      "[[ 1.48437153]\n",
      " [ 0.31812411]\n",
      " [-0.76197545]]\n",
      "[[ 1.60664714]\n",
      " [ 0.27543379]\n",
      " [-0.80178627]]\n",
      "[[ 1.72057624]\n",
      " [ 0.23669265]\n",
      " [-0.84074522]]\n",
      "[[ 1.82717442]\n",
      " [ 0.20176514]\n",
      " [-0.87836247]]\n",
      "[[ 1.92723187]\n",
      " [ 0.17035575]\n",
      " [-0.91447348]]\n",
      "[[ 2.02140044]\n",
      " [ 0.14211607]\n",
      " [-0.94906762]]\n",
      "[[ 2.11023778]\n",
      " [ 0.11669744]\n",
      " [-0.98220216]]\n",
      "[[ 2.19423069]\n",
      " [ 0.09377462]\n",
      " [-1.01396057]]\n",
      "[[ 2.27380877]\n",
      " [ 0.07305471]\n",
      " [-1.04443286]]\n",
      "[[ 2.34935331]\n",
      " [ 0.05427874]\n",
      " [-1.07370691]]\n",
      "[[ 2.42120381]\n",
      " [ 0.03722   ]\n",
      " [-1.10186489]]\n",
      "[[ 2.48966335]\n",
      " [ 0.02168097]\n",
      " [-1.1289822 ]]\n",
      "[[ 2.55500292]\n",
      " [ 0.00748984]\n",
      " [-1.1551275 ]]\n",
      "[[ 2.61746537]\n",
      " [-0.00550276]\n",
      " [-1.18036315]]\n",
      "[[ 2.67726867]\n",
      " [-0.01742689]\n",
      " [-1.20474581]]\n",
      "[[ 2.73460895]\n",
      " [-0.02839594]\n",
      " [-1.22832708]]\n",
      "[[ 2.78966296]\n",
      " [-0.03850899]\n",
      " [-1.25115404]]\n",
      "[[ 2.84259043]\n",
      " [-0.04785285]\n",
      " [-1.27326979]]\n",
      "[[ 2.89353597]\n",
      " [-0.05650373]\n",
      " [-1.29471387]]\n",
      "[[ 2.94263079]\n",
      " [-0.06452873]\n",
      " [-1.31552266]]\n",
      "[[ 2.98999423]\n",
      " [-0.07198709]\n",
      " [-1.33572974]]\n",
      "[[ 3.03573508]\n",
      " [-0.07893124]\n",
      " [-1.35536613]]\n",
      "[[ 3.07995267]\n",
      " [-0.0854077 ]\n",
      " [-1.37446059]]\n",
      "[[ 3.12273795]\n",
      " [-0.09145785]\n",
      " [-1.39303982]]\n",
      "[[ 3.16417431]\n",
      " [-0.09711861]\n",
      " [-1.41112865]]\n",
      "[[ 3.2043384 ]\n",
      " [-0.10242297]\n",
      " [-1.42875023]]\n",
      "[[ 3.24330079]\n",
      " [-0.10740048]\n",
      " [-1.44592615]]\n",
      "[[ 3.28112658]\n",
      " [-0.11207771]\n",
      " [-1.4626766 ]]\n",
      "[[ 3.31787592]\n",
      " [-0.11647855]\n",
      " [-1.47902048]]\n",
      "[[ 3.3536045 ]\n",
      " [-0.12062456]\n",
      " [-1.49497552]]\n",
      "[[ 3.38836395]\n",
      " [-0.12453521]\n",
      " [-1.51055834]]\n",
      "[[ 3.42220225]\n",
      " [-0.12822816]\n",
      " [-1.52578457]]\n",
      "[[ 3.45516399]\n",
      " [-0.13171941]\n",
      " [-1.54066892]]\n",
      "[[ 3.48729073]\n",
      " [-0.13502351]\n",
      " [-1.55522523]]\n",
      "[[ 3.51862125]\n",
      " [-0.13815372]\n",
      " [-1.56946657]]\n",
      "[[ 3.54919176]\n",
      " [-0.14112214]\n",
      " [-1.58340527]]\n",
      "[[ 3.57903617]\n",
      " [-0.14393981]\n",
      " [-1.59705298]]\n",
      "[[ 3.60818619]\n",
      " [-0.14661686]\n",
      " [-1.61042071]]\n",
      "[[ 3.63667162]\n",
      " [-0.14916254]\n",
      " [-1.62351889]]\n",
      "[[ 3.6645204 ]\n",
      " [-0.15158538]\n",
      " [-1.6363574 ]]\n",
      "[[ 3.69175881]\n",
      " [-0.1538932 ]\n",
      " [-1.6489456 ]]\n",
      "[[ 3.7184116 ]\n",
      " [-0.15609322]\n",
      " [-1.6612924 ]]\n",
      "[[ 3.74450206]\n",
      " [-0.15819207]\n",
      " [-1.67340624]]\n",
      "[[ 3.77005219]\n",
      " [-0.16019591]\n",
      " [-1.68529514]]\n",
      "[[ 3.79508273]\n",
      " [-0.1621104 ]\n",
      " [-1.69696673]]\n",
      "[[ 3.8196133]\n",
      " [-0.1639408]\n",
      " [-1.7084283]]\n",
      "[[ 3.84366247]\n",
      " [-0.16569197]\n",
      " [-1.71968675]]\n",
      "[[ 3.8672478 ]\n",
      " [-0.16736844]\n",
      " [-1.7307487 ]]\n",
      "[[ 3.89038595]\n",
      " [-0.16897439]\n",
      " [-1.74162044]]\n",
      "[[ 3.91309271]\n",
      " [-0.17051374]\n",
      " [-1.75230798]]\n",
      "[[ 3.93538308]\n",
      " [-0.17199012]\n",
      " [-1.76281706]]\n",
      "[[ 3.95727131]\n",
      " [-0.17340692]\n",
      " [-1.77315316]]\n",
      "[[ 3.97877094]\n",
      " [-0.17476729]\n",
      " [-1.78332154]]\n",
      "[[ 3.99989488]\n",
      " [-0.17607419]\n",
      " [-1.79332721]]\n",
      "[[ 4.0206554 ]\n",
      " [-0.17733037]\n",
      " [-1.80317498]]\n",
      "[[ 4.0410642 ]\n",
      " [-0.17853842]\n",
      " [-1.81286945]]\n",
      "[[ 4.06113245]\n",
      " [-0.17970075]\n",
      " [-1.82241504]]\n",
      "[[ 4.08087078]\n",
      " [-0.18081964]\n",
      " [-1.83181597]]\n",
      "[[ 4.10028938]\n",
      " [-0.1818972 ]\n",
      " [-1.84107631]]\n",
      "[[ 4.11939797]\n",
      " [-0.18293544]\n",
      " [-1.85019996]]\n",
      "[[ 4.13820583]\n",
      " [-0.18393623]\n",
      " [-1.85919066]]\n",
      "[[ 4.15672186]\n",
      " [-0.18490133]\n",
      " [-1.86805199]]\n",
      "[[ 4.17495457]\n",
      " [-0.1858324 ]\n",
      " [-1.87678744]]\n",
      "[[ 4.19291212]\n",
      " [-0.18673102]\n",
      " [-1.8854003 ]]\n",
      "[[ 4.21060232]\n",
      " [-0.18759865]\n",
      " [-1.8938938 ]]\n",
      "[[ 4.22803267]\n",
      " [-0.18843669]\n",
      " [-1.902271  ]]\n",
      "[[ 4.24521036]\n",
      " [-0.18924644]\n",
      " [-1.91053488]]\n",
      "[[ 4.26214229]\n",
      " [-0.19002915]\n",
      " [-1.91868829]]\n",
      "[[ 4.2788351 ]\n",
      " [-0.190786  ]\n",
      " [-1.92673398]]\n",
      "[[ 4.29529516]\n",
      " [-0.19151808]\n",
      " [-1.93467462]]\n",
      "[[ 4.3115286 ]\n",
      " [-0.19222646]\n",
      " [-1.94251276]]\n",
      "[[ 4.32754131]\n",
      " [-0.19291212]\n",
      " [-1.95025087]]\n",
      "[[ 4.34333896]\n",
      " [-0.19357601]\n",
      " [-1.95789135]]\n",
      "[[ 4.35892701]\n",
      " [-0.19421902]\n",
      " [-1.9654365 ]]\n",
      "[[ 4.37431073]\n",
      " [-0.19484201]\n",
      " [-1.97288855]]\n",
      "[[ 4.38949518]\n",
      " [-0.19544576]\n",
      " [-1.98024964]]\n",
      "[[ 4.40448525]\n",
      " [-0.19603106]\n",
      " [-1.98752186]]\n",
      "[[ 4.41928565]\n",
      " [-0.19659862]\n",
      " [-1.99470721]]\n",
      "[[ 4.43390093]\n",
      " [-0.19714914]\n",
      " [-2.00180766]]\n",
      "[[ 4.44833548]\n",
      " [-0.19768327]\n",
      " [-2.00882507]]\n",
      "[[ 4.46259355]\n",
      " [-0.19820164]\n",
      " [-2.01576128]]\n",
      "[[ 4.47667921]\n",
      " [-0.19870485]\n",
      " [-2.02261804]]\n",
      "[[ 4.49059643]\n",
      " [-0.19919347]\n",
      " [-2.02939708]]\n",
      "[[ 4.50434904]\n",
      " [-0.19966803]\n",
      " [-2.03610004]]\n",
      "[[ 4.51794072]\n",
      " [-0.20012905]\n",
      " [-2.04272853]]\n",
      "[[ 4.53137507]\n",
      " [-0.20057703]\n",
      " [-2.04928411]]\n",
      "[[ 4.54465553]\n",
      " [-0.20101244]\n",
      " [-2.05576829]]\n",
      "[[ 4.55778547]\n",
      " [-0.20143572]\n",
      " [-2.06218254]]\n",
      "[[ 4.57076813]\n",
      " [-0.20184731]\n",
      " [-2.06852828]]\n",
      "[[ 4.58360665]\n",
      " [-0.20224761]\n",
      " [-2.07480688]]\n",
      "[[ 4.59630407]\n",
      " [-0.20263703]\n",
      " [-2.0810197 ]]\n",
      "[[ 4.60886336]\n",
      " [-0.20301592]\n",
      " [-2.08716802]]\n",
      "Output After Training:\n",
      "[[ 0.11035029]\n",
      " [ 0.09193919]\n",
      " [ 0.92564882]\n",
      " [ 0.91041229]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "print(syn0)\n",
    "error_l1 = []\n",
    "l1_revisions = []\n",
    "synapses_revisions = []\n",
    "for iter in range(100):\n",
    "    print(syn0)\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "    \n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "    \n",
    "    error_l1.append(sum(abs(l1_error)))\n",
    "    l1_revisions.append(l1)\n",
    "    synapses_revisions.append(syn0.copy())\n",
    "\n",
    "print(\"Output After Training:\")\n",
    "print(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39533485],\n",
       "       [-0.70648822],\n",
       "       [-0.81532281]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*np.random.random((3, 1)) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2689864 ],\n",
       "       [ 0.36375058],\n",
       "       [ 0.23762817],\n",
       "       [ 0.3262757 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_revisions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11035029],\n",
       "       [ 0.09193919],\n",
       "       [ 0.92564882],\n",
       "       [ 0.91041229]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_revisions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9880842],\n",
       "       [ 0.5      ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(array([[1, 1, 0],[0,0,0]]).dot(synapses_revisions[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-209-f41f39c75d0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msynapses_revisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "np.array([0,0, 1, 1]).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.12025406],\n",
       "       [ 0.50456196],\n",
       "       [-0.85063774]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synapses_revisions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/i008/googledrive/Projects/basics'"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1], \n",
    "             \n",
    "             ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print \"Output After Training:\"\n",
    "print l1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.445131875709\n",
      "Error:0.333333475766\n",
      "Error:0.333333528303\n",
      "Error:0.333333685378\n",
      "Error:0.333333579594\n",
      "Error:0.333333718007\n",
      "Error:0.333334284045\n",
      "Error:0.333337188666\n",
      "Error:0.00311173443307\n",
      "Error:0.0015424644292\n",
      "Error:0.00114603800909\n",
      "Error:0.00094703273195\n",
      "Error:0.000822796884597\n",
      "Error:0.000736072348429\n",
      "Error:0.000671240661306\n",
      "Error:0.000620471217793\n",
      "Error:0.000579355090702\n",
      "Error:0.000545198074995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1,0],\n",
    "            [0,1,1,0],\n",
    "            [1,0,1,0],\n",
    "            [1,1,1,0],\n",
    "            [1,0,0,0],\n",
    "            [1,0,0,0]  \n",
    "            ]\n",
    "    \n",
    "            )\n",
    "                \n",
    "y = np.array(\n",
    "        [[0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        ])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "INPUT_SHAPE = 4\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "input_synapse = 2*np.random.random((INPUT_SHAPE,5)) - 1\n",
    "syn1 = 2*np.random.random((5,11)) - 1\n",
    "syn2 = 2*np.random.random((11,100)) - 1\n",
    "output_synapse = 2*np.random.random((100,4)) - 1\n",
    "\n",
    "for j in range(90000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,input_synapse))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "    l3 = nonlin(np.dot(l2,syn2))\n",
    "    output_layer = nonlin(np.dot(l3, output_synapse))\n",
    "\n",
    "    \n",
    "    \n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    output_error = y - output_layer\n",
    "    output_delta = output_error * nonlin(output_layer, deriv=True)\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    \n",
    "    if (j% 5000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(output_error))))\n",
    "        \n",
    "\n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l3_error = output_delta.dot(output_synapse.T)\n",
    "    l3_delta = l3_error*nonlin(l3, deriv=True)\n",
    "    \n",
    "    \n",
    "    l2_error = l3_delta.dot(syn2.T)\n",
    "    l2_delta = l2_error*nonlin(l2, deriv=True)\n",
    "    \n",
    "    \n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    l1_delta = l1_error*nonlin(l1, deriv=True)\n",
    "    \n",
    "    \n",
    "    l0_error = l1_delta.dot(input_synapse.T)\n",
    "    l0_delta = l0_error*nonlin(l0, deriv=True)\n",
    "\n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    input_synapse += l0.T.dot(l1_delta)\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn2 += l2.T.dot(l3_delta)\n",
    "    output_synapse += l3.T.dot(output_delta)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00170528],\n",
       "       [ 0.99828182],\n",
       "       [ 0.99847086],\n",
       "       [ 0.00153101],\n",
       "       [ 0.99995653],\n",
       "       [ 0.99995653]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.496410031903\n",
      "Error:0.00858452565325\n",
      "Error:0.00578945986251\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array(\n",
    "[\n",
    "    [0,0,1],\n",
    "    [0,1,1],\n",
    "    [1,0,1],\n",
    "    [1,1,1]\n",
    "]\n",
    ")\n",
    "                \n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for j in range(30000):\n",
    "\n",
    "    # Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print(\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.83288099],\n",
       "       [  5.32841436],\n",
       "       [-17.0013023 ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_synapse"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
